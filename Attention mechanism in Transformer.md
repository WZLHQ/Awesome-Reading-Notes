# Attention mechanism in Transformer

## 1. Vanilla attention

* Multi-head attention (MHA), see [paper](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf).

![1767580311273](image/AttentionmechanisminTransformer/1767580311273.png)

## 2. Inference efficiency oriented

* Grouped-Query Attention (GQA), see [paper](https://arxiv.org/pdf/2305.13245).
* Multi-Head Latent Attention (MLA), see [paper](https://arxiv.org/pdf/2405.04434?#page=5.38).

![1767595912502](image/AttentionmechanisminTransformer/1767595912502.png)

## 3. Training efficiency oriented

* Linear attention, see [paper](https://proceedings.mlr.press/v119/katharopoulos20a/katharopoulos20a.pdf).
* flash attention, see [paper](https://proceedings.neurips.cc/paper_files/paper/2022/file/67d57c32e20fd0a7a302cb81d36e40d5-Paper-Conference.pdf).
